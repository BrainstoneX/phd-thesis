\pagenumbering{arabic}
\chapter{Introduction}
Less than 80 years after the invention of the first computer, today we are surrounded by digital technology at every step we take. 
Computers influence and control countless aspects of modern life. 
We have grown so accustomed to digital machines, that we take many of their wonderous abilities for granted. 
One of these wonderous abilities is the following: Computers are able to find the optimal solution to a problem out of an incredibly large amount of possibilities.
For example, suppose you wanted to travel from Paris to Berlin by car. There is an almost infinite amount of different paths from Paris to Berlin. Yet, a clever computer algorithm can select the single unique path which is the fastest among all of them.

This ability of computers to find the optimal solution for a given problem is used in countless areas of modern live: Computers are used to find the cheapest flight schedule for an airline, to find the best investment scheme for a portfolio, to decide which taxis from a taxi company should pick up which customer, to design optimal communication networks, and many, many more problems. Application areas range from Economics, Logistics, Operations Research, Computer Science, Healthcare, Biology, and many other disciplines.

It is important to state that computers do not come with this ability a priori. Instead, specific programs and algorithms need to be developed, to be able to handle the huge amount of possibilities. 
\emph{Combinatorial Optimization} is the scientific field concerned with the question: How do we pick the optimal solution out of a huge (but still finite) amount of possibilities? 
In particular, Combinatorial Optimization tries to understand, what all the previously listed problems have in common, and tries to develop a mathematical theory of these problems and the tools to solve them. Classically, Combinatorial Optimization tries to classify problems as either being tractable (polynomial-time solvable) or intractable (NP-hard). For the intractable problems, it tries to understand what exactly makes them intractable, and whether we can find at least approximate, almost-optimal solutions.

The area of Combinatorial Optimization lies in the intersection between discrete mathematics and theoretical computer science. 
It is a relatively young area of mathematics, which started to appear and take shape approximately in the 1950s. 
As a consequence, even today, still many new aspects and facets of this rich field of study are being researched and discovered. Every of this new developments comes with a re-interpretation or a new perspective of the classical problems in the field. In other words, \emph{generalizations} of the classically important problems are created and analyzed.

\section{Overview of the thesis}

This thesis deals with three kinds of generalizations of classic problems from Combinatorial Optimization. 
The three generalizations reflect on current research trends and sub-areas of Combinatorial Optimization. 
The thesis is roughly split into three parts, corresponding to the three different directions of generalizations that are considered.

\begin{itemize}
\item Part 1: \emph{Robust Optimization} (\cref{ch:recov-selection,ch:multistage-complexity,ch:interdiction}). In Robust Optimization \cite{ben2009robust}, one is concerned with finding good quality solutions, 
which still stay maintain their good quality even if the input parameters of the problem are slightly disturbed. Such solutions are called \emph{robust}.
%This is motivated by the fact that in real life, data upon which decisions rely is often observed to be uncertain: 
%Measurement errors, uncertainty about the future, or other sources of uncertainty lead to the conclusion, that the available data might only be an approximate, but not an exact description of reality. Thus, the need of finding robust solutions arises. 
Robust optimization problems are formulated as min-max expressions. Modern research sees a trend towards investigating \emph{Multi-Stage Robust Optimization}, which solves min-max-min or even more complicated expressions.
 We also consider \emph{Network Interdiction} to be a part of Robust Optimization. 
Network interdiction is concerned with identifying the parts of a network, which are most vulnerable to attack or failure, thus aiding in the design of robust networks. 
In part 1 of the thesis, we consider the following topics in robust optimization: The so-called recoverable representative selection problem in \cref{ch:recov-selection}. The question of $\Sigma^p_k$-hardness of multi-stage robust optimization in \cref{ch:multistage-complexity}. 
Network interdiction problems restricted to interval graphs in \cref{ch:interdiction}.

\item Part 2: \emph{Quadratic Problems and their Linearization}  (\cref{ch:linearization-1,ch:linearization-2}). 
A quadratic problem is an optimization problem whose objective can be written as a quadratic function. 
In contrast to their linear counterpart, quadratic problems are often times intractable. 
This has led researchers to aks the question for special solvable cases of quadratic problems. One of these special cases is the so-called \emph{linearizable} case \cite{bookhold1990contribution}. 
An instance of a quadratic optimization problem is called linearizable, if it can be re-written so that it is equivalent to a linear problem. Linearizations can also help in the design of general Branch \& Bound algorithms. Part 2 of the thesis is concerned with the linearization of the quadratic shortest path problem (QSPP). In \cref{ch:linearization-1}, we present a characterization of the class of universally linearizable directed graphs. In \cref{ch:linearization-2}, we consider the problem of recognizing linearizable instances of the QSPP. We present a novel algorithm for this problem, which runs significantly faster than the current state-of-the-art.

\item Part 3: \emph{Non-Preemptiveness} (\cref{ch:ntp}). In part 3 of the thesis, we are concerned with keeping a network connected as long as possible, by scheduling its edges in a non-preemptive fashion.
This can be seen as a non-preemptive generalization of the problem of \emph{spanning tree packing}, which is a classic problem posed first by Nash-Williams \cite{Nash-Williams1961} to pack as many spanning trees as possible into a given graph. We are the first to consider a constraint of this kind. The result is an interesting mixture of a scheduling problem and a structurally rich graph-theoretic problem.
\end{itemize}

This thesis is based on the following research papers:

\begin{enumerate}[(i)]
\item M. Goerigk, S. Lendl, and L. Wulf. \textit{Recoverable robust representatives selection problems with discrete budgeted uncertainty.} European Journal of Operational Research 303(2), 567--580, 2022. \cite{goerigk2022recoverable}

\item M. Goerigk, S. Lendl, and L. Wulf. \textit{On the Complexity of Robust Multi-Stage Problems in the Polynomial Hierarchy.} arXiv preprint, 2022 \cite{goerigk2022complexity}.

\item H. P. Hoang, S. Lendl, and L. Wulf.  \textit{Assistance and Interdiction Problems on Interval Graphs.} arXiv preprint, 2021 \cite{hoang2021assistance}. To appear in: Discrete Applied Mathematics.

\item E. Çela, B. Klinz, S. Lendl, J. B. Orlin, G. J. Woeginger, and L. Wulf. \textit{Linearizable
special cases of the quadratic shortest path problem.} In Graph-Theoretic Concepts in Computer Science -- 47th
International Workshop, WG 2021, Warsaw, Poland \cite{cela2021linearizable}.

\item E. Çela, B. Klinz, S. Lendl, G. J. Woeginger, and L. Wulf. \textit{A linear time algorithm
for linearizing quadratic and higher-order shortest path problems.} In Integer Programming and Combinatorial Optimization -- 24th
International Conference, IPCO 2023, Madison, WI, USA \cite{cela2023linear}.

\item S. Lendl, G. J. Woeginger, and L. Wulf. \textit{Non-preemptive tree packing.} Algorithmica, 85(3), 783-804, 2023 \cite{lendl2023nonpreemptive}. A conference version appeared in the 32nd Workshop on Combinatorial Algorithms, IWOCA 2021. The conference version received the best student paper award.
 \end{enumerate}

The rest of the introduction proceeds as follows: Background information and motivation is given in \cref{sec:motivation-background}. In the subsequent  \cref{sec:thesis-results}, the main results of the thesis are explained in detail.

\section{Motivation and background}
\label{sec:motivation-background}

We assume that the reader is familiar with basic concepts from Combinatorial Optimization, in particular with basic graph theory and graph algorithms. An introduction to Combinatorial Optimization can be found in the books of Korte and Vygen \cite{korte2006combinatorial}, and Schrijver \cite{schrijver2003combinatorial}. We furthermore assume knowledge of basic concepts from complexity theory, like the concepts of NP-completeness and polynomial-time algorithms. Specifically in \cref{ch:multistage-complexity}, we require the notion of the polynomial hierarchy and $\Sigma^p_k$-completeness. An introduction to these topics is found in the book of Arora and Barak \cite{arora2009computational}. Finally, we also assume knowledge of linear programming and duality. An introduction to these topics can be found in the book of Matou{\v{s}}ek and Gärtner \cite{matouvsek2007understanding}.

\paragraph*{Combinatorial Optimization Problem.}
Given some ground set $E = \fromto{e_1}{e_n}$, we let $\X \subseteq \set{0,1}^n$ denote the set of \emph{feasible solutions}. 
We assume here that the feasible solutions are encoded as $0$-$1$-vectors (i.e.\ $x_i = 1$ corresponds to element $e_i$ being part of the feasible solution, and $x_i=0$ corresponds to $e_i$ not being part of the feasible solution). 
For example, $\X$ could encode the set of all spanning trees of a graph, the set of all paths between two vertices, of the set of all traveling salesman tours, etc\dots If there is some \emph{objective function} (also called \emph{cost function}) $f : \set{0,1}^n \to \R$, then the problem
\[
\min_{x \in \X} f(x)
\]
is referred to as a \emph{combinatorial optimization problem}. The cost function $f$ is called \emph{linear} if there exists some vector $c \in \R^n$ such that $f(x) = c^tx$ for all $x \in \set{0,1}^n$. This means that the cost of element $e_i$ is given by $c_i$.

In some chapters, we use the set-theoretic notation for linear cost functions instead. In this case, we consider a cost function $c : E \to \R$, which assigns a cost to every single element. The cost of a subset $E' \subseteq E$ is then given by
\[ c(E') := \sum_{e \in E'}c(e). \]



\paragraph*{Robust Optimization.}

In real-world applications, decision-makers are faced with the problem that the data upon which they base their decision on is uncertain. For example, some uncertainty may be subject to measurement errors of the measurement device. Some of the data entries may depend on the future, and can only be roughly predicted, hence they are subject to prediction errors. Some of the data entries may describe properties of real-world procedures, which can not be executed with perfect mathematical precision, hence they are subject to implementation errors.

These errors may have substantial effect on the quality of the solution. For example, in a study performed by Ben-Tal and Nemirovski \cite{ben2000robust}, the case of linear programming was considered: 
Taking a linear program (abbreviated LP) from the well-known NETLIB library, they disturbed the coefficients of the LP by 0.1\%. 
The authors solved the disturbed LP (which represents the data known to the decision maker) to optimality and compared the solution quality to the original LP (which represents the true data, unknown to the decision maker). 
In one case, the optimal solution to the disturbed LP was infeasible for the orgininal LP by a relative constraint violation of 450\%. 
Similar results are reported in the average case. This schowcases the need for robust solutions. The idea is that possibly one can sacrifice a small amount of optimality in the solution, in order to compute a solution which is not necessarily the best for the known data, but in exchange is stable against disturbations of the input data. Such a solution is called \emph{robust}.



Mathematically, the robust optimization approach to problems of this type is to formulate an \textit{uncertainty set} $\mathcal{U}$. This set contains all the \enquote{reasonable-to-expect} cost functions, which one wants to prepare against. One then takes a worst case approach and computes the \emph{robust problem}

\[
	\min_{x \in \X} \max_{c \in \cU} c(x).
\]

The different cost functions $c \in \cU$ are also called \emph{scenarios}. By changing the size of the set $\cU$, we can influence for how many scenarios we want to prepare against, and thus control the amount of robustness of the solution (for the price of a worse objective value). The original (non-robust) problem to find $\min_{x \in \X} c(x)$ is called \emph{nominal problem}.

Note that we protect ourselves against \emph{all} possible scenarios equally, by assuming that the worst-case scenario will arise. This conservative approach has the advantage that no prior probability distribution needs to be assumed about the data (agnostic approach), that it often leads to algorithmically tractable problems, and that theoretical analysis is often possible. This stands in contrast to so-called \emph{stochastic programming} \cite{birge2011introduction}, where a probability distribution is assumed, and the solution is optimized for e.g.\ the expected value. Both methods have advantages and drawbacks (compare \cite{ben2009robust} for a detailed discussion).  
 
Since its inception in the early 2000s, the field of robust optimization has developed so rapidly, that it is impossible to give a complete overview here. We refer the reader to the book by Kouvelis and Yu \cite{kouvelis2013robust}, the book by Ben-Tal, El Ghaoui, and Nemirovski~\cite{ben2009robust} and the survey by Bertsimas, Brown and Caramaras~\cite{bertsimas2011theory}. 

\paragraph*{Budgeted Uncertainty Sets.}
The choice of uncertainty set $\cU$ is crucial for robust optimization. Many different choices have been suggested [todo]. In this thesis, we mainly consider \emph{budgeted uncertainty}. This is a type of uncertainty which allows up $\Gamma$ parameters to deviate from their nominal value for some parameter $\Gamma \in \N$. Budgeted uncertainty is also called \emph{$\Gamma$-uncertainty} in the literature. We distinguish between \emph{discrete} budgeted uncertainty and \emph{continuous} budgeted uncertainty.

Formally, for given constants $\underline{c}_i \geq 0$ and $d_i \in \R$ for all $i \in \fromto{1}{n}$ and some integer $\Gamma \geq 0$, the set $\cU^d_\Gamma$ of \emph{discrete budgeted uncertainty} is defined as
\[
\cU^d_\Gamma = \set{ \pmb c \in \R^V : c_i = \underline{c}_i + \delta_id_i,\  \delta_i \in \set{0,1}\ \forall i \in [n],\ \sum_{i \in V}\delta_i \leq \Gamma }.
\]
In other words, $\cU^d_\Gamma$ contains those cost functions where at most $\Gamma$ entries deviate from their nominal cost $\underline{c}_i$. We define $\overline{c}_i = \underline{c}_i + d_i$ as the deviated $i$-th value. We can also consider the case, where the deviation is not binary, but can be split partially between different coeeficients. Formally, \emph{continuous budgeted uncertainty} is defined as
\[
\cU^c_\Gamma = \set{ \pmb c \in \R^V : c_i = \underline{c}_i + \delta_id_i,\  \delta_i \in [0,1]\ \forall i \in [n],\ \sum_{i \in V}\delta_i \leq \Gamma }.
\]
Note that the only difference between the two formulations is the range of $\delta_i$.

Budgeted uncertainty sets were introduced by Bertsimas and Sim \cite{bertsimas2003robust,bertsimas2004price}. 
The high citation count on these two papers shows that discrete budgeted uncertainty has become very successful in the robust optimization literature. 
A famous result shown by Bertsimas and Sim in the same paper is that when using budgeted uncertainty (discrete or continuous), the robust min-max problem can be solved by decomposing it into $O(n)$ nominal problems, hence it is compuationally tractable.

Just listing some of its use cases, budgeted uncertainty sets are used in [todo]. Specifically in combinatorial optimization, there are many papers applying them to different problems [todo].

Further history of budgeted uncertainty sets, as well as some of its generalizations can be found in [todo]

\paragraph*{Multi-Stage Robust Optimization.}
The conservative approach of robust optimization can be considered a drawback. One possible way to address this drawback is by considering \emph{multi-stage robust optimization}. 
These are problems which enable the decision maker to undertake a partial corrective action after the uncertainty has been revealed. They are formulated as min-max-min equations.
As classical robust optimization has been well researched and is generally understood quite well, multistage robust optimization has gained some attention in recent years [todo: cite] In this thesis, we focus on two different kinds of multi-stage robust optimization:  \emph{two-stage adjustable} and \emph{recoverable} robust optimization.

In two-stage adjustable robust optimization \cite{ben2004adjustable}, we distinguish between two types of variables. The values of \emph{here-and-now (first-stage)} variables need to be decided beforehand. We then receive the information which scenario from the uncertainty set has been realized, before we decide on the value of \emph{wait-and-see (second-stage)} variables. 
This models a natural two-stage decision process, where in the first stage important decisions need to be made without full knowledge of the future, and in the second stage some restricted decisions can be made to adjust to the encountered scenario.
 A recent survey on adjustable robust optimization can be found in \cite{yanikouglu2019survey}.
There are different models of two-stage adjustable robust optimization. In this thesis, we consider a model which is also used by other authors in the context of combinatorial optimization [todo: cite]. 
We assume that there is some set of feasible solutions $\X \subseteq \set{0,1}^n$, and that the first-stage variables $x \in \set{0,1}^n$ together with the second-stage variables $y \in \set{0,1}^n$ must combine to a feasible solution. This is expressed by the constraint $x + y \in \X$. 
We let $\X' \subseteq \set{0,1}^n$ be the set of permissible first-stage decisions and we let $\X(x) := \set{y \in \set{0,1}^n : x + y \in \X}$ be the set of possible ways the first-stage decision $x$ can be completed to a feasible solution. 
To address the fact where a first-stage solution $x$ is chosen which can not be completed (i.e.\ $\X(x) = \emptyset$), we define $\min \emptyset := \infty$. This means that such a choice of $x$ should be avoided. Formally, for some objective function $f(x,y,c)$, the two-stage adjustable robust problem is given by

\[\min_{x \in \X'} \max_{c \in \cU} \min_{y \in \X(x)} f(x,y,c).\]

Two-stage adjustable robust optimization has been considered for many combinatorial problems, including [todo]. 

A variant of this approach is recoverable robust optimization \cite{liebchen2009concept}. Here, the idea is that we have to propose some initial solution $x$, which should already resemble a feasible solution. 
After the reveal of the uncertain cost function $c$, we are allowed to react to the new information by proposing a new solution $y$, but we must make sure that $x$ is similar to $y$. Formally, for some abstract distance function $\dist(\cdot)$ and parameter $k \in \N$, we make sure that $\dist(x,y) \leq k$. This approach is motivated by train scheduling problems: The actual train schedule $y$ can deviate from the initially planned schedule $x$ to accomodate for delays, but $x$ and $y$ should not be too dissimilar, in order to satisfy customers \cite{liebchen2009concept}. Formally, for a set $\X \subseteq \set{0,1}^n$ of feasible solutions, and $\X(x) := \set{y \in \X : \dist(x,y) \leq k}$ we consider

\[\min_{x \in \X} \max_{c \in \cU} \min_{y \in \X(x)} f(x,y,c).\]

Recoverable robust optimization has been considered for many combinatorial problems, including [todo]. 

For both variants, the sub-problem to compute for a given first-stage solution $x$ the quantity $\max_{c \in \cU} \min_{y \in \X(x)} f(x,y,c)$ is called the \emph{adversarial problem} $\textsc{Adv}(x)$. 
This is inspired by the fact that robust optimization can be interpreted as a game between two players: The first player finds a minimizing $x$, while the second player, also called the adversary, has the opposite goal and selects a cost function $c \in \cU$. The first player responds to the second player by choosing some $y \in \X(x)$.

\paragraph*{Quadratic Shortest Path Problem.}
A combinatorial optimization problem is \emph{quadratic}, if its cost function $f : \set{0,1}^n \to \R$ can be written as $f(x) = x^tQx$ for some $n \times n$ matrix $Q$. The \emph{quadratic shortest path problem (QSPP)} takes as input a directed graph, two vertices $s,t$, and a matrix $Q \in \R^{m \times m}$, where $m$ denotes the number of edges in $G$. It asks for the path from $s$ to $t$ of smallest quadratic cost $f(x)$ (where $x \in \R^m$ is the incidence vector of the edges of the path). 

The QSPP arises 
in network optimization  problems where costs are associated with both single arcs and pairs of arcs.
This includes 
variants of stochastic and time-dependent route planing  problems  
\cite{nie2009reliable,sen2001mean,sivakumar1994variance}
and network design problems 
\cite{murakami1997restoration,gamvros2006satellite}. 
For an overview on applications of the QSPP see \cite{huSo2020,rostami2018}.
The QSPP is NP-hard~\cite{rostami2018}.
It is also difficult from the practical point of view~\cite{huSo2020}.

\paragraph*{Linearization.}
Since quadratic problems are often times intractable, researchers look for special solvable cases. One of these cases is the linearizable case. An instance $(G,s,t,Q)$ of the QSPP is \emph{linearizable} in the sense of Bookhold \cite{bookhold1990contribution}, if there exists a vector $c \in \R^m$ such that for all $s$-$t$-paths $P$ with incidence vector $x = x_P$ we have the equation 
\[x^tQx = c^tx. \]
In other words, the instance can be expressed equivalently as a linear optimization problem.

Linearizations may appear quite exotic at first, but they arise naturally as special cases of the QSPP with restricted cost matrix~\cite{huSo2018}. They also have applications in understanding and designing lower bounds in general branch \& bound algorithms for quadratic optimization problems \cite{huSo2021}. 

For this reason, linearizability has been studied for many quadratic problems, including the quadratic assignment problem
 \cite{CeDeWo2016,Erdogan2006,ErTa2007,ErTa2011,kabadi2011n,punnen2013linear,waddellcharacterizing}, 
 the quadratic minimum spanning tree problem \cite{CuPu2018,sotirov2021quadratic},   the quadratic TSP \cite{PuWaWo2017}, and  the quadratic cycle cover problem \cite{deMeSo2020}.  

%\paragraph*{Spanning Tree Packing}

\section{Results of the thesis}
\label{sec:thesis-results}

In this section, we extensively describe the results obtained in each chapter.

\paragraph*{Recoverable representative selection.}
In \cref{ch:recov-selection}, we consider recoverable robust optimization in combination with the \emph{selection problem} and the \emph{(multi-)representative selection problem}. 
These are two simple combinatorial optimization problems, which are trivial to solve without robustness, but may become challenging with the introduction of robustness. 
The selection problem is defined by having the set of feasible solutions $\X = \set{x \in \set{0,1}^n : \sum_{i=1}^n x_i = p}$ for some $p \in \N$ (i.e.\ a single cardinality constraint). 
The multi-representative selection problem is defined for a partition $T_1 \dotunion \dots \dotunion T_K = \fromto{1}{n}$ by the set of feasible solutions $\X = \set{ \pmb{x}\in \{0,1\}^n : \sum_{i\in T_j} x_i = p_j\ \forall j \in [K] }$ for some numbers $p_1,\dots,p_K \in \N$. If $p_j = 1$ for all $j \in [K]$, then the multi-representative selection problem is simply called the \emph{representative selection} problem (it can be thought of the problem as selecting the representative of each part).

These two problems are special cases of many well-known problems, for example the spanning tree problem or the shortest path problem in series-parallel graphs. Hence a hardness or inapproximability proof immediately propagates to these harder problems. For this reason, the selection problem is a popular toy problem in robust optimization to test the properties of proposed models \cite{averbakh2001complexity,conde2004improved,dolgui2012min,deineko2013complexity,goerigk2019robust} 
.

In \cref{ch:recov-selection}, we consider the above problems in combination with recoverable robust optimization with discrete budgeted uncertainty $\cU^d_\Gamma$. 
This was first considered for the representative selection problem in 2011 in the PhD thesis of Büsing \cite{busing2011phd}, but its complexity was left as an open case. 
This was revisited again in 2018 for the selection problem by Chassein et al.\ \cite{chassein2018recoverable}, but again no complexity results could be derived. 

We are able to settle the open question by showing NP-completeness of both the robust selection and robust representative selection problem. We also show the  analogous hardness result for two-stage adjustable (instead of recoverable) selection.
The hardness proof is based on a careful balancing of two strategies. We furthermore show that the adversarial problem for recoverable representative multi-selection can be solved in polynomial time. This is based on a technically challenging argument involving LP duality, kink points of piecewise linear functions, and dynamic programming.
We show that we can solve recoverable representative selection in polynomial time in the very special case that $k=1$, where $k$ is the recoverability parameter. It is an open question whether this result extends to $k = O(1)$. Finally, some computational experiments are performed.
 
\paragraph*{$\Sigma^p_k$-hardness of multi-stage robust optimization.}
In \cref{ch:multistage-complexity}, we are concerned with the computational complexity of multi-stage robust optimization problems. Since such problems are formulated with alternating min/max quantifiers, they naturally fall into a higher stage of the polynomial hierarchy by Stockmeyer \cite{stockmeyer1976polynomial}. Despite this, almost no hardness results with respect to the polynomial hierarchy are known for such problems. In this chapter, we examine the hardness of robust two-stage adjustable and robust recoverable optimization with budgeted uncertainty sets. Our main technical contribution is the introduction of a technique tailored to prove $\Sigma^p_k$-hardness of such problems. We highlight a difference between continuous ($\cU^c_\Gamma$) and discrete ($\cU^d_\Gamma$) budgeted uncertainty: In the discrete case, indeed a wide range of problems becomes complete for the third stage of the polynomial hierarchy. We highlight the TSP, independent set, and vertex cover problems as examples of this behavior. However, in the continuous case this does not happen and all problems remain in the first stage of the hierarchy. Finally, if we allow the uncertainty to not only affect the objective, but also multiple constraints, then this distinction disappears and even in the continuous case we encounter hardness for the third stage of the hierarchy. This shows that even robust problems which are already NP-complete can still exhibit a significant computational difference between column-wise and row-wise uncertainty.

\paragraph*{Network interdiction on interval graphs.}
\emph{Network Interdiction} \cite{NetworkInterdictProblemsBookChapter} is the study of figuring out which parts of a network are most vulnerable to attack or failure. The area of network interdiction has grown tremendously in recent years \cite{criticalNodeDetectionSurvey}. 
We consider network interdiction as closely related to robust optimization, since both disciplines involve solving min-max equations. One popular type of network interdiction is the \emph{most vital nodes problem} [todo:cite]. Here, the goal is to inhibit some graph parameter $\pi$ by a maximal amount by deleting only few nodes of a graph (for example, destroy all large cliques of a graph by deleting only few nodes). Diner et al. \cite{diner2018contractionDeletionBlockers} considered the most vital nodes problem for $H$-free and perfect graphs and obtained many different results. For interval graphs and parameter $\pi = \alpha$, they were unable to settle the complexity and left it as an open question \cite[(Q2)]{diner2018contractionDeletionBlockers}.

In \cref{ch:interdiction}, we introduce a novel framework of graph modifications specific to interval graphs. We study interdiction problems with respect to these graph modifications. 
Given a list of original intervals, each interval has a replacement interval such that either the replacement contains the original, or the original contains the replacement. 
The interdictor is allowed to replace up to $k$ original intervals with their replacements. 
Using this framework we also study the contrary of interdiction problems which we call assistance problems. We study these problems for the independence number, the clique number, shortest paths, and the scattering number. 
We obtain polynomial time algorithms for 6 out of the 8 the studied problems. Via easy reductions, it follows that on interval graphs, the most vital nodes problem with respect to shortest path, independence number and Hamiltonicity can be solved in polynomial time.

Our results are based on in some cases very technically challenging dynamic programs. As a special case, we answer both questions (Q1) and (Q2) from \cite{diner2018contractionDeletionBlockers}. The proof is based on the duality of independence number and clique cover number in perfect graphs. We want to highlight in particular the result from \cref{sec:hamiltonicity}. 
Here it is shown that restricted to interval graphs the \emph{most vital nodes problem for Hamiltonicity} can be solved in polynomial time. 
In this problem, we are given an interval graph with the promise that it contains a Hamiltonian cycle. The question is, for some given $k$, whether we can delete $k$ vertices such that after the deletion there is no Hamiltonian cycle anymore. 

We find it surprising that this problem can be solved in polynomial time, since in general graphs the Hamiltonian cycle problem is already NP-complete, and the most vital nodes variant often times makes a problem even harder. Our proof is based on the characterization of Hamiltonicity of an interval graph using the scattering number by Jung \cite{jung1978scat}, and the dynamic programming formulation of a certain non-deterministic process. Our result actually holds in a much larger generality in the framework which we introduce.

\paragraph*{Linearization I.}
In \cref{ch:linearization-1}, we consider linearizable special cases of the quadratic shortest path problem (QSPP). If the QSPP on a directed graph is linearizable under all possible choices of the arc interaction costs, the graph is called universally linearizable. This is motivated by an earlier result of Hu and Sotirov, who provided one specific example of a universally linearizable directed graph \cite{huSo2018}.

We completely characterize the class of universally linearizable directed graphs.
In fact, we provide several equivalent characterizations, which are centered around the structure of source-to-sink paths and around certain forbidden subgraphs. Our characterizations lead to fast and simple recognition algorithms for universally linearizable graphs. 

Furthermore, we establish the intractability of deciding whether a concrete instance of the QSPP (with a given graph and given arc interaction costs) is linearizable. We call this problem LinQSPP. We show that LinQSPP is coNP-complete. Interestingly, showing coNP-hardness is the easy part of the proof, but the proof that the problem is even contained in the class coNP is nontrivial. For this, we make use of Farkas' lemma \cite{Farkas1902} and polyhedric geometry.


\paragraph*{Linearization II.}
In \cref{ch:linearization-2}, we again consider the problem LinQSPP.
While LinQSPP is coNP-complete by the results of the previous chapter, it can be solved in polynomial time when restricted to acyclic graphs.
In fact, Hu and Sotirov first showed in 2018 that for acyclic directed grid graphs $G_{a,b}$ the problem LinQSPP can be solved in time $O(^3b^2 + a^2b^3)$ \cite{huSo2018}. Later, they showed that LinQSPP can be solved for all acyclic directed graphs in time $O(nm^3)$ \cite{huSo2021}. Here $n,m$ denote the number of vertices and edges of the graph, respectively.

In \cref{ch:linearization-2}, we provide a novel linear time algorithm for the LinQSPP on acyclic digraphs which runs in time $O(m^2)$, improving over the previously best algorithm by a factor $O(nm)$. We complement this with a trivial lower bound of $\Omega(m^2)$.

The algorithm is based on a new insight revealing that the linearizability of the QSPP for acyclic digraphs can be seen as a local property. Roughly speaking, the instance as a whole is linearizable if and only if every small part of the instance is linearizable. We deem this surprising, since this reduces a global property to a local property. Still, there may be exponentially many of these small parts. Therefore, we use additional algorithmic techniques to check all these parts and obtain a polynomial-time algorithm. The idea for this new local-global characterization is based on insights gained in \cref{ch:linearization-1}.

Our approach furthermore extends to the more general higher-order shortest path problem. Here, we obtain a runtime of $O(m^d)$ together with a matching lower bound $\Omega(m^d)$ for all orders $d \in \N, d \geq 2$. 
Finally, we show how to compute in polynomial time a basis of the linear subspace of all linearizable degree-$d$ cost functions for a given fixed graph.

\paragraph*{Non-preemptive tree packing.}
In \cref{ch:ntp}, we consider \emph{nonpreemptive tree packing}. This problem is a mixture of a scheduling problem and a structural tree packing problem.
An instance of the non-preemptive tree packing problem consists of an undirected graph $G$ together with a weight $w(e)$ for every edge $e$. The goal is to activate every edge $e$ for some time interval of length $w(e)$, such that the activated edges keep $G$ connected for the longest possible overall time. We derive a variety of results on this problem. The problem is strongly NP-hard even on graphs of treewidth $2$, and it does not allow a polynomial time approximation scheme (unless P=NP). Furthermore, we discuss the performance of a simple greedy algorithm, and we construct and analyze a number of parameterized and exact algorithms.
We can pinpoint the complexity of this problem parameterized by the objective value down to a small gap: On the one hand, it is NP-complete to decide if the graph can be connected for 7 time units. On the other hand, if 7 is replaced by 3, this problem becomes polynomial-time solvable. Therefore, the only open cases that remain are the cases of $\set{4,5,6}$.

\section{A note on notation}

[todo]