\pagenumbering{arabic}
\chapter{Introduction}
Less than 80 years after the invention of the first computer, today we are surrounded by digital technology at every step we take. 
Computers influence and control countless aspects of modern life. 
We have grown so accustomed to digital machines, that we take many of their wonderous abilities for granted. 
One of these wonderous abilities is the following: Computers are able to find the optimal solution to a problem out of an incredibly large amount of possibilities.
For example, suppose you wanted to travel from Paris to Berlin by car. There is an almost infinite amount of different paths from Paris to Berlin. Yet, a clever computer algorithm can select the single unique path which is the fastest among all of them.

This ability of computers to find the optimal solution for a given problem is used in countless areas of modern live: Computers are used to find the cheapest flight schedule for an airline, to find the best investment scheme for a portfolio, to decide which taxis from a taxi company should pick up which customer, to design optimal communication networks, and many, many more problems. Application areas range from Economics, Logistics, Operations Research, Computer Science, Healthcare, Biology, and many other disciplines.

It is important to state that computers do not come with this ability a priori. Instead, specific programs and algorithms need to be developed, to be able to handle the huge amount of possibilities. 
\emph{Combinatorial Optimization} is the scientific field concerned with the question: How do we pick the optimal solution out of a huge (but still finite) amount of possibilities? 
In particular, Combinatorial Optimization tries to understand, what all the previously listed problems have in common, and tries to develop a mathematical theory of these problems and the tools to solve them. Classically, Combinatorial Optimization tries to classify problems as either being tractable (polynomial-time solvable) or intractable (NP-hard). For the intractable problems, it tries to understand what exactly makes them intractable, and whether we can find at least approximate, almost-optimal solutions.

The area of Combinatorial Optimization lies in the intersection between discrete mathematics and theoretical computer science. 
It is a relatively young area of mathematics, which started to appear and take shape approximately in the 1950s. 
As a conseuqence, even today, still many new aspects and facets of this rich field of study are being researched and discovered. Every of this new developments comes with a re-interpretation or a new perspective of the classical problems in the field. In other words, \emph{generalizations} of the classically important problems are created and analyzed.

\section{Overview of the Thesis}

This thesis deals with three kind of generalizations of classic problems from Combinatorial Optimization. 
The three generalizations reflect on current research trends and sub-areas of Combinatorial Optimization. 
The thesis is roughly split into three parts, corresponding to the three different directions of generalizations that are considered.

\begin{itemize}
\item Part 1: \emph{Robust Optimization} (\cref{ch:recov-selection,ch:multistage-complexity,ch:interdiction}). In Robust Optimization, one is concerned with finding good quality solutions, 
which still stay maintain their good quality even if the input parameters of the problem are slightly disturbed. Such solutions are called \emph{robust}.
%This is motivated by the fact that in real life, data upon which decisions rely is often observed to be uncertain: 
%Measurement errors, uncertainty about the future, or other sources of uncertainty lead to the conclusion, that the available data might only be an approximate, but not an exact description of reality. Thus, the need of finding robust solutions arises. 
Robust optimization problems are formulated as min-max expressions. Modern research sees a trend towards investigating \emph{Multi-Stage Robust Optimization}, which solves min-max-min or even more complicated expressions. We also consider \emph{Network Interdiction} to be a part of Robust Optimization. Network interdiction is concerned with identifying the parts of a network, which are most vulnerable to attack or failure, thus aiding in the design of robust networks. 

\item Part 2: \emph{Quadratic Problems and their Linearization}  (\cref{ch:linearization-1,ch:linearization-2}). 
A quadratic problem is an optimization problem whose objective can be written as a quadratic function. 
In contrast to their linear counterpart, quadratic problems are often times intractable. 
This has led researchers to aks the question for special solvable cases of quadratic problems. One of these special cases is the so-called \emph{lienarizable} case. 
An instance of a quadratic optimization problem is called linearizable, if it can be re-written so that it is equivalent to a linear problem. Linearizations can also help in the design of general Branch \& Bound algorithms. 

\item Part 3: \emph{Non-Preemptiveness} (\cref{ch:ntp}). In part 3 of the thesis, we are concerned with keeping a network connected as long as possible, by scheduling its edges in a non-preemptive fashion.
This can be seen as a non-preemptive generalization of the problem of \emph{spanning tree packing}, which is a classic problem posed first by Nash-Williams to pack as many spanning trees as possible into a given graph. We are the first to consider a constraint of this kind. The result is an interesting mixture of a scheduling problem and a structurally rich graph-theoretic problem.
\end{itemize}

We provide a short overview of the single chapters in this thesis and their respective main results. After the short overview, further background information and motivation is given in \cref{sec:motivation-background}. In the subsequent  \cref{sec:thesis-results}, the main results of the thesis are explained in more detail.

\paragraph*{Part 1: Robust Optimization}
\begin{itemize}
\item In \cref{ch:recov-selection}, we consider so-called \emph{Recoverable Robust Optimization} (a kind of multi-stage robust optimization) in conjunction with the so-called representative selection problem and so-called discrete budgeted uncertainty. 
This is a problem which is trivial without the presence of any robustness, but the addition of robustness makes it challenging. 
Determining the computational hardness of the specific variant that we consider has been an open question since 2012 [cite], which we now resolve by showing NP-completeness. 
We also show that the corresponding adversarial problem can be solved in polynomial time.

\item   In \cref{ch:multistage-complexity}, we consider multi-stage robust optimization. We make a compelling case that the natural complexity class for many problems studied in the literature is the class $\Sigma^p_3$ from the polynomial hierarchy. Despite this, very few $\Sigma^p_3$-completeness results are known. We make a first step by showing several results of this kind.

\item In \cref{ch:interdiction}, we consider network interdiction problems specifically with respect to interval graphs. We introduce a framework for interdiction on interval graphs, which allows one to shrink or expand intervals. We show that the well-known \emph{most vital nodes} problem is a special case of the framework. Even though network interdiction problems are often times NP-complete or even harder, in our case we obtain polynomial-time algorithms for 6 of the 8 cases studied. This is done employing technically challenging dynamic programs. One of our results answers an open question by Diner et al. [cite].
\end{itemize}

\paragraph*{Part 2: Linearization}
\begin{itemize}
\item In \cref{ch:linearization-1}, we consider linearization of the quadratic shortest path problem. In an earlier paper, Sotirov and Hu gave an example of a certain directed graph with a peculiar property, which we call unviersally linearizable. Motivated by this example, we give a complete characterization of the class of universally linearizable directed graphs. 

\item In \cref{ch:linearization-2}, we continue the work started in \cref{ch:linearization-1}. 
We describe an algorithm that solves the linearization problem of the quadratic shortest path problem on acyclic directed graphs in running time $O(m^2)$. 
There is also a trivial tight lower bound of $\Omega(m^2)$, making our algorithm linear-time. 
In contrast, the fastest previous algorithm had a running time of $O(nm^3)$ [cite], hence we improve the state-of-the art by a factor of $O(mn)$. Our algorithm is based on a surprising new insight, stating that linearizability of an acyclic directed graph can be understood as a local property.  Our approach generalizes to the case of cost functions of degree higher than $d$, where we also obtain a linear-time algorithm.
\end{itemize}

\paragraph*{Part 3: Non-preemptiveness}
\begin{itemize}
\item In \cref{ch:ntp}, we consider the problem of keeping a graph connected for a maximal amount of time, by scheduling its edges in a non-preemptive way. 
%Specifically, every edge $e$ has a weight $w(e)$ attached to it, and can be scheduled during a non-interrupted interval of length $w(e)$. The goal is to maximize the time during which the scheduled edges span all the vertices. 
We show that the problem is an interesting mixture between a scheduling and a structural graph-theoretic problem. We can pinpoint the complexity of this problem parameterized by the objective value down to a small gap: On the one hand, it is NP-complete to decide if the graph can be connected for 7 time units. On the other hand, if 7 is replaced by 3, this problem becomes polynomial-time solvable. Therefore, the only open cases that remain are the cases of $\set{4,5,6}$.
\end{itemize}


\section{Motivation and Background}
\label{sec:motivation-background}

We assume that the reader is familiar with basic concepts from Combinatorial Optimization, in particular with basic graph theory and graph algorithms. An introduction to Combinatorial Optimization can be found in the books of Korte and Vygen \cite{korte2006combinatorial}, and Schrijver \cite{schrijver2003combinatorial}. We furthermore assume knwoledge of basic concepts from complexity theory, like the concepts of NP-completeness and polynomial-time algorithms. Specifically in \cref{ch:multistage-complexity}, we require the notion of the polynomial hierarchy and $\Sigma^p_k$-completeness. An introduction to these topics is found in the book of Arora and Barak \cite{arora2009computational}. Finally, we also assume knowledge of linear programming and duality. An introduction to these topics can be found in the book of Matou{\v{s}}ek and GÃ¤rtner \cite{matouvsek2007understanding}.

\paragraph*{Combinatorial Optimization Problem}
Given some ground set $E = \fromto{e_1}{e_n}$, we let $\X \subseteq \set{0,1}^n$ denote the set of \emph{feasible solutions}. 
We assume here that the feasible solutions are encoded as $0$-$1$-vectors (i.e.\ $x_i = 1$ corresponds to element $e_i$ being part of the feasible solution, and $x_i=0$ corresponds to $e_i$ not being part of the feasible solution). 
For example, $\X$ could encode the set of all spanning trees of a graph, the set of all paths between two vertices, of the set of all traveling salesman tours, etc\dots If there is some \emph{objective function} (also called \emph{cost function}) $f : \set{0,1}^n \to \R$, then the problem
\[
\min_{x \in \X} f(x)
\]
is referred to as a \emph{Combinatorial Optimization problem}. The cost function $f$ is called \emph{linear} if there exists some vector $c \in \R^n$ such that $f(x) = c^tx$ for all $x \in \set{0,1}^n$. This means that the cost of element $e_i$ is given by $c_i$.

In some chapters, we use the set-theoretic notation for linear cost functions instead. In this case, we consider a cost function $c : E \to \R$, which assigns a cost to every single element. The cost of a subset $E' \subseteq E$ is then given by
\[ c(E') := \sum_{e \in E'}c(e). \]



\paragraph*{Robust Optimization}

In real-world applications, decision-makers are faced with the problem that the data upon which they base their decision on is uncertain. For example, some uncertainty may be subject to measurement errors of the measurement device. Some of the data entries may depend on the future, and can only be roughly predicted, hence they are subject to prediction errors. Some of the data entries may describe properties of real-world procedures, which can not be executed with perfect mathematical precision, hence they are subject to implementation errors.

These errors may have substantial effect on the quality of the solution. For example, in a study performed by Ben-Tal and Nemirovski \cite{ben2000robust}, the case of linear programming was considered: 
Taking a linear program (abbreviated LP) from the well-known NETLIB library, they disturbed the coefficients of the LP by 0.1\%. 
The authors solved the disturbed LP (which represents the data known to the decision maker) to optimality and compared the solution quality to the original LP (which represents the true data, unknown to the decision maker). 
In one case, the optimal solution to the disturbed LP was infeasible for the orgininal LP by a relative constraint violation of 450\%. 
Similar results are reported in the average case. This schowcases the need for robust solutions. The idea is that possibly one can sacrifice a small amount of optimality in the solution, in order to compute a solution which is not necessarily the best for the known data, but in exchange is stable against disturbations of the input data. Such a solution is called \emph{robust}.



Mathematically, the robust optimization approach to problems of this type is to formulate an \textit{uncertainty set} $\mathcal{U}$. This set contains all the \enquote{reasonable-to-expect} cost functions, which one wants to prepare against. One then takes a worst case approach and computes the \emph{robust problem}

\[
	\min_{x \in \X} \max_{c \in \cU} c(x).
\]

The different cost functions $c \in \cU$ are also called \emph{scenarios}. By changing the size of the set $\cU$, we can influence for how many scenarios we want to prepare against, and thus control the amount of robustness of the solution (for the price of a worse objective value).

Note that we protect ourselves against \emph{all} possible scenarios equally, by assuming that the worst-case scenario will arise. This conservative approach has the advantage that no prior probability distribution needs to be assumed about the data (agnostic approach), that it often leads to algorithmically tractable problems, and that theoretical analysis is often possible. This stands in contrast to so-called \emph{stochastic programming} \cite{birge2011introduction}, where a probability distribution is assumed, and the solution is optimized for e.g.\ the expected value. Both methods have advantages and drawbacks (compare \cite{ben2009robust} for a detailed discussion).  
 
Since its inception in the early 2000s, the field of robust optimization has developed so rapidly, that it is impossible to give a complete overview here. We refer the reader to the book by Ben-Tal, El Ghaoui, and Nemirovski~\cite{ben2009robust} and the survey by Bertsimas, Brown and Caramaras~\cite{bertsimas2011theory}.

\paragraph*{Budgeted Uncertainty Sets.}
The choice of uncertainty set $\cU$ is crucial for robust optimization. Many different choices have been suggested [todo]. In this thesis, we mainly consider \emph{budgeted uncertainty}. This is a type of uncertainty which allows up $\Gamma$ parameters to deviate from their nominal value for some parameter $\Gamma \in \N$. Budgeted uncertainty is also called \emph{$\Gamma$-uncertainty} in the literature. We distinguish between \emph{discrete} budgeted uncertainty and \emph{continuous} budgeted uncertainty.

Formally, for given constants $\underline{c}_i \geq 0$ and $d_i \in \R$ for all $i \in \fromto{1}{n}$ and some integer $\Gamma \geq 0$, the set $\cU^d_\Gamma$ of \emph{discrete budgeted uncertainty} is defined as
\[
\cU^d_\Gamma = \set{ \pmb c \in \R^V : c_i = \underline{c}_i + \delta_id_i,\  \delta_i \in \set{0,1}\ \forall i \in [n],\ \sum_{i \in V}\delta_i \leq \Gamma }.
\]
In other words, $\cU^d_\Gamma$ contains those cost functions where at most $\Gamma$ entries deviate from their nominal cost $\underline{c}_i$. We define $\overline{c}_i = \underline{c}_i + d_i$ as the deviated $i$-th value. We can also consider the case, where the deviation is not binary, but can be split partially between different coeeficients. Formally, \emph{continuous budgeted uncertainty} is defined as
\[
\cU^c_\Gamma = \set{ \pmb c \in \R^V : c_i = \underline{c}_i + \delta_id_i,\  \delta_i \in [0,1]\ \forall i \in [n],\ \sum_{i \in V}\delta_i \leq \Gamma }.
\]
Note that the only difference between the two formulations is the range of $\delta_i$.

Budgeted uncertainty sets were introduced by Bertsimas and Sim \cite{bertsimas2003robust,bertsimas2004price}. 
The high citation count on these two papers shows that discrete budgeted uncertainty has become very successful in the robust optimization literature. 
A famous result shown by Bertsimas and Sim in the same paper is that when using budgeted uncertainty (discrete or continuous), the robust min-max problem can be solved by decomposing it into $O(n)$ nominal problems, hence it is compuationally very tractable.

Just listing some of its use cases, budgeted uncertainty sets are used in [todo]. Specifically in combinatorial optimization, there are many papers applying them to different problems [todo].

Further history of budgeted uncertainty sets, as well as some of its generalizations can be found in [todo]

\paragraph*{Multi-Stage Robust Optimization}
The conservative approach of robust optimization can be considered a drawback. One possible way to address this drawback is by considering \emph{multi-stage robust optimization}. 
These are problems which enable the decision maker to undertake a partial corrective action after the uncertainty has been revealed. They are formulated as min-max-min equations.
As classical robust optimization has been well researched and is generally understood quite well, multistage robust optimization has gained some attention in recent years [todo: cite] In this thesis, we focus on two different kinds of multi-stage robust optimization:  \emph{two-stage adjustabele} and \emph{recoverable} robust optimization.

In two-stage adjustable robust optimization \cite{ben2004adjustable}, we distinguish between two types of variables. The values of here-and-now (first-stage) variables need to be decided beforehand. We then receive the information which scenario from the uncertainty set has been realized, before we decide on the value of wait-and-see (second-stage) variables. 
This models a natural two-stage decision process, where in the first stage important decisions need to be made without full knowledge of the future, and in the second stage some decisions can be made to adjust to the encountered scenario.
 A recent survey on adjustable robust optimization can be found in \cite{yanikouglu2019survey}.
There are different models of two-stage adjustable robust optimization. In this thesis, we consider a model which is also used by other authors in the context of combinatorial optimization [todo: cite]. 
We assume that there is some set of feasible solutions $\X \subseteq \set{0,1}^n$, and that the first-stage variables $x \in \set{0,1}^n$ together with the second-stage variables $y \in \set{0,1}^n$ must combine to a feasible solution. This is expressed by the constraint $x + y \in \X$. 
We let $\X'$ be the set of permissible first-stage decisions and we let $\X(x) := \set{y \in \set{0,1}^n : x + y \in \X}$ be the set of possible ways the first-stage decision $x$ can be completed to a feasible solution. 
To address the fact where a first-stage solution $x$ is chosen which can not be completed (i.e.\ $\X(x) = \emptyset$), we define $\min \emptyset := \infty$. This means that such a choice of $x$ should be avoided. Formally, for some objective function $f(x,y,c)$, the two-stage adjustable robust problem is given by

\[\min_{x \in \X'} \max_{c \in \cU} \min_{y \in \X(x)} f(x,y,c).\]

Two-stage adjustable robust optimization has been considered for many combinatorial problems, including [todo]. 

A variant of this approach is recoverable robust optimization \cite{liebchen2009concept}. Here, the idea is that we have to propose some initial solution $x$, which should already resemble a feasible solution. 
After the reveal of the uncertain cost function $c$, we are allowed to react to the new information by proposing a new solution $y$, but we must make sure that $x$ is similar to $y$. Formally, for some abstract distance function $\dist(\cdot)$ and parameter $k \in \N$, we make sure that $\dist(x,y) \leq k$. This approach is motivated by train scheduling problems: The actual train schedule $y$ can deviate from the initially planned schedule $x$ to accomodate for delays, but $x$ and $y$ should not be too dissimilar, in order to satisfy customers \cite{liebchen2009concept}. Formally, for a set $\X \subseteq \set{0,1}^n$ of feasible solutions, and $\X(x) := \set{y \in \X : \dist(x,y) \leq k}$ we consider

\[\min_{x \in \X} \max_{c \in \cU} \min_{y \in \X(x)} f(x,y,c).\]

Recoverable robust optimization has been considered for many combinatorial problems, including [todo]. 

\paragraph*{Quadratic Shortest Path Problem.}
A combinatorial optimization problem is \emph{quadratic}, if its cost function $f : \set{0,1}^n \to \R$ can be written as $f(x) = x^tQx$ for some $n \times n$ matrix $Q$. The \emph{quadratic shortest path problem (QSPP)} takes as input a directed graph, two vertices $s,t$, and a matrix $Q \in \R^{m \times m}$, where $m$ denotes the number of edges in $G$. It asks for the path from $s$ to $t$ of smallest quadratic cost $f(x)$ (where $x \in \R^m$ is the incidence vector of the edges of the path). 

The QSPP arises 
in network optimization  problems where costs are associated with both single arcs and pairs of arcs.
This includes 
variants of stochastic and time-dependent route planing  problems  
\cite{nie2009reliable,sen2001mean,sivakumar1994variance}
and network design problems 
\cite{murakami1997restoration,gamvros2006satellite}. 
For an overview on applications of the QSPP see \cite{huSo2020,rostami2018}.
The QSPP is NP-hard~\cite{rostami2018}.
It is also difficult from the practical point of view~\cite{huSo2020}.

\paragraph*{Linearization.}
Since quadratic problems are often times intractable, researchers look for special solvable cases. One of these cases is the linearizable case. An instance $(G,s,t,Q)$ of the QSPP is \emph{linearizable} in the sense of Bookhold \cite{bookhold1990contribution}, if there exists a vector $c \in \R^m$ such that for all $s$-$t$-paths $P$ with incidence vector $x = x_P$ we have the equation 
\[x^tQx = c^tx. \]
In other words, the instance can be expressed equivalently as a linear optimization problem.

Linearizations may appear quite exotic at first, but they arise naturally as special cases of the QSPP with restricted cost matrix~\cite{huSo2018}. They also have applications in understanding and designing lower bounds in general branch \& bound algorithms for quadratic optimization problems \cite{huSo2021}. 

For this reason, linearizability has been studied for many quadratic problems, including the quadratic assignment problem
 \cite{CeDeWo2016,Erdogan2006,ErTa2007,ErTa2011,kabadi2011n,punnen2013linear,waddellcharacterizing}, 
 the quadratic minimum spanning tree problem \cite{CuPu2018,sotirov2021quadratic},   the quadratic TSP \cite{PuWaWo2017}, and  the quadratic cycle cover problem \cite{deMeSo2020}.  

%\paragraph*{Spanning Tree Packing}

\section{Results of the Thesis}
\label{sec:thesis-results}

\section{Highlights of the Thesis}
\label{sec:thesis-highlights}